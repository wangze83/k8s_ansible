#
# Cluster Config
#
docker_root_dir: /var/lib/docker
enable_cluster_alerting: false
enable_cluster_monitoring: true
enable_network_policy: false
fleet_workspace_name: fleet-default
local_cluster_auth_endpoint:
  enabled: true
#
# Rancher Config
#
rancher_kubernetes_engine_config:
  addon_job_timeout: 300
  authentication:
    strategy: x509|webhook
  authorization:
    mode: rbac
  bastion_host:
    ssh_agent_auth: false
  ignore_docker_version: false
  #
  #   # 当前仅支持nginx的ingress
  #   # 设置`provider: none`禁用ingress控制器
  #   # 通过node_selector可以指定在某些节点上运行ingress控制器，例如:
  #    provider: nginx
  #    node_selector:
  #      app: ingress
  #
  ingress:
    provider: none
  kubernetes_version: v1.19.7-rancher1-1
  monitoring:
    provider: metrics-server
    replicas: 1
  #
  #   # 如果您在AWS上使用calico
  #
  #    network:
  #      plugin: calico
  #      calico_network_provider:
  #        cloud_provider: aws
  #
  #   # 指定flannel网络接口
  #
  #    network:
  #      plugin: flannel
  #      flannel_network_provider:
  #      iface: eth1
  #
  #   # 指定canal网络插件的flannel网络接口
  #
  #    network:
  #      plugin: canal
  #      canal_network_provider:
  #        iface: eth1
  #
  network:
    mtu: 0
    plugin: none
  private_registries:
    - is_default: true
      url: harbor.wz.cn
  restore:
    restore: false
  #
  #  # 自定义服务参数，仅适用于Linux环境
  #    services:
  #      kube-api:
  #        service_cluster_ip_range: 10.1.1.1/16
  #        extra_args:
  #          watch-cache: true
  #      kube-controller:
  #        cluster_cidr: 10.1.1.1/16
  #        service_cluster_ip_range: 10.1.1.1/16
  #        extra_args:
  #          # 修改每个节点子网大小(cidr掩码长度)，默认为24，可用IP为254个；23，可用IP为510个；22，可用IP为1022个；
  #          node-cidr-mask-size: 24
  #          # 控制器定时与节点通信以检查通信是否正常，周期默认5s
  #          node-monitor-period: '5s'
  #          # 当节点通信失败后，再等一段时间kubernetes判定节点为notready状态。这个时间段必须是kubelet的nodeStatusUpdateFrequency(默认10s)的N倍，其中N表示允许kubelet同步节点状态的重试次数，默认40s。
  #          node-monitor-grace-period: '20s'
  #          # 再持续通信失败一段时间后，kubernetes判定节点为unhealthy状态，默认1m0s。
  #          node-startup-grace-period: '30s'
  #          # 再持续失联一段时间，kubernetes开始迁移失联节点的Pod，默认5m0s。
  #          pod-eviction-timeout: '1m'
  #      kubelet:
  #        cluster_domain: cluster.local
  #        cluster_dns_server: 10.1.1.1
  #        # 扩展变量
  #        extra_args:
  #          # 与apiserver会话时的并发数，默认是10
  #          kube-api-burst: '30'
  #          # 与apiserver会话时的 QPS,默认是5
  #          kube-api-qps: '15'
  #          # 修改节点最大Pod数量
  #          max-pods: '250'
  #          # secrets和configmaps同步到Pod需要的时间，默认一分钟
  #          sync-frequency: '3s'
  #          # kubelet默认一次拉取一个镜像，设置为false可以同时拉取多个镜像，前提是存储驱动要为overlay2，对应的Docker也需要增加下载并发数
  #          serialize-image-pulls: false
  #          # 拉取镜像的最大并发数，registry-burst不能超过registry-qps ，仅当registry-qps大于0(零)时生效，(默认10)。如果registry-qps为0则不限制(默认5)。
  #          registry-burst: '10'
  #          registry-qps: '0'
  #          # 以下配置用于配置节点资源预留和限制
  #          cgroups-per-qos: 'true'
  #          cgroup-driver: cgroupfs
  #          # 以下两个参数指明为相关服务预留多少资源，仅用于调度，不做实际限制
  #          system-reserved: 'memory=300Mi'
  #          kube-reserved: 'memory=2Gi'
  #          enforce-node-allocatable: 'pods'
  #          # 硬驱逐阈值，当节点上的可用资源少于这个值时，就会触发强制驱逐。强制驱逐会强制kill掉POD，不会等POD自动退出。
  #          eviction-hard: 'memory.available<300Mi,nodefs.available<10%,imagefs.available<15%,nodefs.inodesFree<5%'
  #          # 软驱逐阈值
  #          ## 以下四个参数配套使用，当节点上的可用资源少于这个值时但大于硬驱逐阈值时候，会等待eviction-soft-grace-period设置的时长；
  #          ## 等待中每10s检查一次，当最后一次检查还触发了软驱逐阈值就会开始驱逐，驱逐不会直接Kill POD，先发送停止信号给POD，然后等待eviction-max-pod-grace-period设置的时长；
  #          ## 在eviction-max-pod-grace-period时长之后，如果POD还未退出则发送强制kill POD
  #          eviction-soft: 'memory.available<500Mi,nodefs.available<50%,imagefs.available<50%,nodefs.inodesFree<10%'
  #          eviction-soft-grace-period: 'memory.available=1m30s'
  #          eviction-max-pod-grace-period: '30'
  #          ## 当处于驱逐状态的节点不可调度，当节点恢复正常状态后
  #          eviction-pressure-transition-period: '5m0s'
  #        extra_binds:
  #          - "/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins"
  #          - "/etc/iscsi:/etc/iscsi"
  #          - "/sbin/iscsiadm:/sbin/iscsiadm"
  #      etcd:
  #        # 修改空间配额为$((4*1024*1024*1024))，默认2G,最大8G
  #        extra_args:
  #          quota-backend-bytes: '4294967296'
  #          auto-compaction-retention: 240 #(单位小时)
  #      kubeproxy:
  #        extra_args:
  #        # 默认使用iptables进行数据转发
  #          proxy-mode: ""    # 如果要启用ipvs，则此处设置为`ipvs`
  #
  services:
    etcd:
      backup_config:
        enabled: true
        interval_hours: 12
        retention: 10
        safe_timestamp: true
      creation: 12h
      extra_args:
        election-timeout: '1000'
        heartbeat-interval: '200'
        listen-metrics-urls: 'http://0.0.0.0:2381'
        quota-backend-bytes: '8589934592'
        auto-compaction-retention: 240
      extra_binds:
        - '/etc/localtime:/etc/localtime'
      gid: 0
      retention: 72h
      snapshot: false
      uid: 0
    kube_api:
      always_pull_images: false
      extra_args:
        enable-aggregator-routing: 'true'
      extra_binds:
        - '/etc/localtime:/etc/localtime'
      pod_security_policy: false
      service_cluster_ip_range: 192.168.0.0/16
      service_node_port_range: 30000-32767
    kube_controller:
      cluster_cidr: 10.1.1.1/18
      extra_args:
        cluster-signing-cert-file: /etc/kubernetes/ssl/kube-ca.pem
        cluster-signing-key-file: /etc/kubernetes/ssl/kube-ca-key.pem
        experimental-cluster-signing-duration: 87600h0m0s
        pod-eviction-timeout: 60s
      extra_binds:
        - '/etc/localtime:/etc/localtime'
      service_cluster_ip_range: 192.168.0.0/16
    kubelet:
      cluster_dns_server: 192.168.0.2
      cluster_domain: idc2.local
      extra_args:
        kube-reserved=cpu: '1000m,memory=4096Mi'
        max-pods: '33'
        read-only-port: '10255'
        system-reserved: 'cpu=1000m,memory=4096Mi'
        volume-plugin-dir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      extra_binds:
        - >-
          /usr/libexec/kubernetes/kubelet-plugins/volume/exec:/usr/libexec/kubernetes/kubelet-plugins/volume/exec
        - '/etc/localtime:/etc/localtime'
      fail_swap_on: false
      generate_serving_certificate: false
    kubeproxy:
      extra_args:
        conntrack-min: '4194304'
        ipvs-min-sync-period: 5s
        ipvs-scheduler: rr
        ipvs-sync-period: 10s
        proxy-mode: ipvs
      extra_binds:
        - '/etc/localtime:/etc/localtime'
    scheduler:
      extra_binds:
        - '/etc/localtime:/etc/localtime'
  ssh_agent_auth: false
  upgrade_strategy:
    drain: false
    max_unavailable_controlplane: '1'
    max_unavailable_worker: 10%
    node_drain_input:
      delete_local_data: true
      force: false
      grace_period: -1
      ignore_daemon_sets: true
      timeout: 60
scheduled_cluster_scan:
  enabled: false
  scan_config:
    cis_scan_config:
      debug_master: false
      debug_worker: false
      override_benchmark_version: rke-cis-1.5
      profile: permissive
  schedule_config:
    cron_schedule: 0 0 * * *
    retention: 24
windows_prefered_cluster: false
